{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import splitfolders\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from cub_dataloader_good import CUB, denormalize, normalize, visualize\n",
    "\n",
    "print(\"PyTorch Version: \", torch.__version__)\n",
    "print(\"Torchvision Version: \", torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = './CUB_200_2011/'\n",
    "num_classes = 200\n",
    "num_epochs = 30\n",
    "\n",
    "feature_extract = True\n",
    "\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "print(f'setting everything to seed {seed}')\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "resnet = models.resnet101(pretrained=True)\n",
    "\n",
    "# # Use the model object to select the desired layer\n",
    "# layer = model._modules.get('avgpool')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "resnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize():\n",
    "    return transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "\n",
    "def denormalize(img):\n",
    "    imagenet_stats = np.array([[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]])\n",
    "    return img*imagenet_stats[1] + imagenet_stats[0]\n",
    "\n",
    "def show_image(img):\n",
    "  img = img.transpose(1,2,0)\n",
    "  img= denormalize(img)\n",
    "  plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the image transformations - normalization based on ImageNet parameters\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    normalize(),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('./CUB_200_2011/CUB_200_2011')\n",
    "labels = pd.read_csv(PATH/\"image_class_labels.txt\", header=None, sep=\" \")\n",
    "labels.columns = [\"id\", \"label\"]\n",
    "labels.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pd.read_csv(PATH/\"images.txt\", header=None, sep=\" \")\n",
    "images.columns = [\"id\", \"name\"]\n",
    "images.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = pd.read_csv(\"./xlsa17/data/CUB/trainvalclasses.txt\", header=None, sep=\".\")\n",
    "test_classes = pd.read_csv(\"./xlsa17/data/CUB/testclasses.txt\", header=None, sep=\".\")\n",
    "image_class_labels = pd.read_csv(PATH/\"image_class_labels.txt\", header=None, sep=\" \")\n",
    "# images = pd.read_csv(PATH/\"images.txt\", header=None, sep=\" \")\n",
    "\n",
    "train_images = images.merge(image_class_labels, left_on='id', right_on=0, how=\"inner\").merge(train_classes, how=\"inner\", left_on=1, right_on=0)\n",
    "train_images['is_train'] = 1\n",
    "test_images = images.merge(image_class_labels, left_on='id', right_on=0, how=\"inner\").merge(test_classes, how=\"inner\", left_on=1, right_on=0)\n",
    "test_images['is_train'] = 0\n",
    "\n",
    "# combine train and test images\n",
    "train_test = pd.concat([train_images, test_images], axis=0)\n",
    "train_test.drop(train_test.columns[[0, 2, 3, 4, 5, 6]], axis=1, inplace=True)\n",
    "train_test.columns = [\"id\", \"is_train\"]\n",
    "train_test = train_test.sort_values(by='id')\n",
    "train_test = train_test.reset_index(drop=True)\n",
    "\n",
    "# # Separate dataframes for is_train == 1 and is_train == 0\n",
    "# df_train_1 = train_test[train_test['is_train'] == 1]\n",
    "# df_train_0 = train_test[train_test['is_train'] == 0]\n",
    "\n",
    "# # Randomly select half of the rows from each dataframe to drop\n",
    "# drop_indices_1 = np.random.choice(df_train_1.index, size=int(df_train_1.shape[0]/2), replace=False)\n",
    "# drop_indices_0 = np.random.choice(df_train_0.index, size=int(df_train_0.shape[0]/2), replace=False)\n",
    "\n",
    "# # Drop these rows\n",
    "# train_test = train_test.drop(drop_indices_1)\n",
    "# train_test = train_test.drop(drop_indices_0)\n",
    "# images = images.drop(drop_indices_1)\n",
    "# images = images.drop(drop_indices_0)\n",
    "# labels = labels.drop(drop_indices_1)\n",
    "# labels = labels.drop(drop_indices_0)\n",
    "\n",
    "print(train_test.shape)\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pd.read_csv(PATH/\"classes.txt\", header=None, sep=\" \")\n",
    "classes.columns = [\"id\", \"class\"]\n",
    "classes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [x for x in classes[\"class\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributes = pd.read_csv(\n",
    "#     PATH/\"attributes/image_attribute_labels.txt\", \n",
    "#     header=None, \n",
    "#     delim_whitespace=True, \n",
    "#     on_bad_lines=lambda x: [x[0], x[1], x[2], x[3], \"-1\"], \n",
    "#     engine='python'\n",
    "# )\n",
    "# attributes.drop(attributes.columns[3:], axis=1, inplace=True)\n",
    "# attributes.columns = [\"id\", \"attribute\", \"is_present\"]\n",
    "\n",
    "class_attributes = pd.read_csv(PATH/\"attributes/class_attribute_labels_continuous.txt\", header=None, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PATH = Path('./CUB_200_2011/CUB_200_2011')\n",
    "\n",
    "print(labels.shape)\n",
    "print(train_test.shape)\n",
    "print(images.shape)\n",
    "\n",
    "train_dataset = CUB(PATH, labels, train_test, images, train= True, transform= True)\n",
    "valid_dataset = CUB(PATH, labels, train_test, images, train= False, transform= False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, num_workers=4)\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(train_loader, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "resnet = models.resnet101(pretrained=True)\n",
    "resnet.to(device)\n",
    "\n",
    "layer = resnet._modules.get('avgpool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(img):\n",
    "    img = image_transform(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "    my_embedding = torch.zeros(2048)\n",
    "    def copy_data(m, i, o):\n",
    "        my_embedding.copy_(o.data.squeeze())\n",
    "    h = layer.register_forward_hook(copy_data)\n",
    "    resnet(img)\n",
    "    h.remove()\n",
    "    return my_embedding\n",
    "\n",
    "# get_vector implementation that works with batches\n",
    "# imgs is a tensor of shape (batch_size, 3, 224, 224) \n",
    "def get_vectors(imgs):\n",
    "    imgs = imgs.to(device)\n",
    "    my_embeddings = torch.zeros(imgs.shape[0], 2048)\n",
    "    def copy_data(m, i, o):\n",
    "        my_embeddings.copy_(o.data.squeeze())\n",
    "    h = layer.register_forward_hook(copy_data)\n",
    "    resnet(imgs)\n",
    "    h.remove()\n",
    "    return my_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "single_image = x[0]\n",
    "single_label = y[0]\n",
    "\n",
    "print(f\"Image: {single_image}\")\n",
    "print(f\"Image shape: {single_image.shape}\")\n",
    "print(f\"Label: {single_label}\")\n",
    "\n",
    "inp = single_image.numpy().transpose(1,2,0)\n",
    "inp = denormalize(inp)\n",
    "plt.imshow(inp)\n",
    "\n",
    "\n",
    "single_image.unsqueeze_(0)\n",
    "\n",
    "# Make sure the image tensor is on the same device as the model\n",
    "single_image = single_image.to(next(resnet.parameters()).device)\n",
    "\n",
    "# Make sure image dtype is the same as model's expected input\n",
    "single_image = single_image.float()\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "resnet.eval()\n",
    "\n",
    "# Apply the model to the image\n",
    "output = resnet(single_image)\n",
    "\n",
    "# Get the index corresponding to the maximum score and the maximum score itself.\n",
    "_, pred = torch.max(output.data, 1)\n",
    "print(f\"Predicted class index: {pred.item()}\")\n",
    "print(f\"Predicted class: {categories[pred.item()]}\")\n",
    "print(f\"Actual class label: {categories[single_label]}\")\n",
    "\n",
    "print(type(output))\n",
    "print(output)\n",
    "print(output.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN_CLASSES = len(train_classes)\n",
    "N_CLASSES = len(train_classes) + len(test_classes)\n",
    "N = N_TRAIN_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement SE-GZSL networks\n",
    "# Implement semantic encoder\n",
    "def make_semantic_encoder():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(2048, 4096),\n",
    "        nn.LeakyReLU(0.02)\n",
    "    )\n",
    "\n",
    "def make_residual_encoder():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(2048, 4096),\n",
    "        nn.LeakyReLU(0.02)\n",
    "    )\n",
    "\n",
    "def make_decoder():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(4096*2, 2048),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "def make_classifier():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(4096, N_CLASSES),\n",
    "        nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "\n",
    "# Implement generator\n",
    "def make_generator():\n",
    "    # ? how to get the noise vector in here?\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(312, 2048),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "# Implement critic\n",
    "def make_critic():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(4096, 1),\n",
    "        nn.LeakyReLU(0.02)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import subprocess as sp\n",
    "import os\n",
    "\n",
    "def get_gpu_memory():\n",
    "    command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    return memory_free_values\n",
    "\n",
    "def cosine_similarity(list_1, list_2):\n",
    "  cos_sim = dot(list_1, list_2) / (norm(list_1) * norm(list_2))\n",
    "  return cos_sim\n",
    "\n",
    "def similiarity_loss(semantic_encoder_outputs):\n",
    "  # semantic encoder outputs is a list of [tensor[4096], tensor[1]] pairs\n",
    "  # the first element represents the semantic encoder output, the second element the class label\n",
    "  # we want to maximize the cosine similarity between semantic encoder outputs of the same class\n",
    "  # while minimizing the cosine similarity between semantic encoder outputs of different classes\n",
    "\n",
    "  # first, we need to group the semantic encoder outputs by class\n",
    "  # we can do this by creating a dictionary with the class label as key and a list of semantic encoder outputs as value\n",
    "  # we can then iterate over the dictionary and calculate the cosine similarity between all pairs of semantic encoder outputs\n",
    "  # we can then take the mean of the cosine similarities between all pairs of semantic encoder outputs of the same class\n",
    "  # and the mean of the cosine similarities between all pairs of semantic encoder outputs of different classes\n",
    "  # and return the difference between the two means as the loss\n",
    "\n",
    "  # create dictionary with class label as key and list of semantic encoder outputs as value\n",
    "  # with torch.no_grad():\n",
    "  cos_similarity = torch.nn.CosineSimilarity(dim=0).to(device)\n",
    "\n",
    "  # print(get_gpu_memory())\n",
    "  semantic_encoder_outputs_dict = {}\n",
    "  for semantic_encoder_output in semantic_encoder_outputs:\n",
    "    class_label = semantic_encoder_output[1]\n",
    "    if class_label in semantic_encoder_outputs_dict:\n",
    "      semantic_encoder_outputs_dict[class_label].append(semantic_encoder_output[0])\n",
    "    else:\n",
    "      semantic_encoder_outputs_dict[class_label] = [semantic_encoder_output[0]]\n",
    "  # print(get_gpu_memory())\n",
    "  \n",
    "  similiarity_loss = 0\n",
    "  for label, outputs in semantic_encoder_outputs_dict.items():\n",
    "    # print(get_gpu_memory())\n",
    "    same_class_similarity = 0\n",
    "    different_class_similarity = 0\n",
    "\n",
    "    # print(len(outputs))\n",
    "\n",
    "    for z_i in outputs:\n",
    "      # print(z_i)\n",
    "      # calculate same class similarity\n",
    "      for z_j in outputs:\n",
    "        # print(z_j)\n",
    "        # skip same element\n",
    "        if torch.equal(z_i, z_j):\n",
    "          # print('equal')\n",
    "          continue\n",
    "\n",
    "        cos_sim = cos_similarity(z_i, z_j)\n",
    "        same_class_similarity += torch.exp(cos_sim)\n",
    "        # print('add same')\n",
    "\n",
    "      # calculate different class similarity\n",
    "      for label_prime, outputs_prime in semantic_encoder_outputs_dict.items():\n",
    "        # skip same class\n",
    "        if label_prime == label:\n",
    "          continue\n",
    "\n",
    "        for z_j_prime in outputs_prime:\n",
    "          cos_sim = cos_similarity(z_i, z_j_prime)\n",
    "          different_class_similarity += torch.exp(cos_sim)\n",
    "    # print('same class similarity')\n",
    "    # print(same_class_similarity)\n",
    "    # print('different class similarity')\n",
    "    # print(different_class_similarity)\n",
    "    similiarity_loss += torch.log(torch.div(same_class_similarity, different_class_similarity))\n",
    "    # print('emptying cache' + '='*20)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "  return torch.divide(similiarity_loss, len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mi_estimators import CLUB, CLUBForCategorical, InfoNCE\n",
    "\n",
    "def mi_loss(semantic_output, residual_output, labels):\n",
    "    # print(f\"output.shape: {output.shape}\")\n",
    "    # print(f\"target.shape: {target.shape}\")\n",
    "\n",
    "    # labels has shape [32] and contains the class label ids for each image in the batch\n",
    "    # class_attributes has shape [200, 312] and contains the class attributes for each class\n",
    "    # class_attributes[labels] has shape [32, 312] and contains the class attributes for each image in the batch\n",
    "    cuda_class_attributes = torch.from_numpy(class_attributes.to_numpy()).to(device)\n",
    "    batch_class_attributes = cuda_class_attributes[labels].to(torch.float32)\n",
    "\n",
    "    lambda_s = 0.1 # TODO: tune\n",
    "    lambda_r = 0.1 # TODO: tune\n",
    "    # InfoNCE takes x_dim, y_dim and hidden_size\n",
    "    info_nce = InfoNCE(4096, 1, 4096).to(device)\n",
    "    club = CLUBForCategorical(4096, 312, 4096).to(device)\n",
    "    semantic_mi_loss = -lambda_s * info_nce(semantic_output, batch_class_attributes)\n",
    "    residual_mi_loss = lambda_r * club(residual_output, batch_class_attributes)\n",
    "    return semantic_mi_loss + residual_mi_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TotalLoss(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(TotalLoss, self).__init__()\n",
    "\n",
    "    def forward(self, reconstructed_image_features, semantic_outputs, residual_output, labels):\n",
    "        loss = 0\n",
    "        # loss = mi_loss(semantic_outputs, residual_output, labels) # + nn.NLLLoss()(output, target)\n",
    "        loss += similiarity_loss(semantic_outputs)\n",
    "        print(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "mat = scipy.io.loadmat('./xlsa17/data/CUB/att_splits.mat')\n",
    "mat = np.array(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement custom model\n",
    "class SE_GZSL(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.semantic_encoder = make_semantic_encoder()\n",
    "        self.residual_encoder = make_residual_encoder()\n",
    "        self.decoder = make_decoder()\n",
    "\n",
    "    def forward(self, image_features):\n",
    "        semantic_output = self.semantic_encoder(image_features)\n",
    "        residual_output = self.residual_encoder(image_features)\n",
    "        reconstructed_image_feature = self.decoder(torch.cat((semantic_output, residual_output), dim=1))\n",
    "        return reconstructed_image_feature, semantic_output, residual_output\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.generator = make_generator()\n",
    "\n",
    "    def forward(self, noise_vector, attribute_vector):\n",
    "        generated_image_features = self.generator(torch.cat((noise_vector, attribute_vector), dim=1))\n",
    "        return generated_image_features\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.critic = make_critic()\n",
    "\n",
    "    def forward(self, image_features):\n",
    "        critic_output = self.critic(image_features)\n",
    "        return critic_output\n",
    "    \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.classifier = make_classifier()\n",
    "\n",
    "    def forward(self, image_features):\n",
    "        classifier_output = self.classifier(image_features)\n",
    "        return classifier_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop for SE-GZSL\n",
    "\n",
    "# Initialize the model for this run\n",
    "se_gzsl = SE_GZSL()\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(se_gzsl)\n",
    "\n",
    "# Send the model to GPU\n",
    "se_gzsl = se_gzsl.to(device)\n",
    "\n",
    "# initialize the optimizer\n",
    "optimizer = optim.Adam(se_gzsl.parameters(), lr=0.001)\n",
    "criterion = TotalLoss()\n",
    "\n",
    "sem_out = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # initialize tensor that holds all semantic_output/label pairs\n",
    "    semantic_outputs = []\n",
    "\n",
    "    print('passing')\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device).to(torch.float32)\n",
    "        labels = labels.to(device)\n",
    "        # print(labels)\n",
    "\n",
    "        # print(images)\n",
    "        # print(images.shape)\n",
    "        # print(labels)\n",
    "        # print(labels.shape)\n",
    "\n",
    "        # extract image features using resnet\n",
    "        image_features = get_vectors(images).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        reconstructed_image_features, semantic_output, residual_output = se_gzsl(image_features)\n",
    "\n",
    "        # reconstructed_image_features\n",
    "        # semantic_output = semantic_output\n",
    "        # residual_output\n",
    "        semantic_output = semantic_output.cpu()\n",
    "        labels = labels.cpu()\n",
    "\n",
    "        # create pairs of semantic output and label and save them to a tensor\n",
    "        for i in range(len(semantic_output)):\n",
    "          semantic_outputs.append([semantic_output[i], labels[i].item()])\n",
    "\n",
    "        # write the above in a more torchy way\n",
    "        # semantic_outputs.append(torch.cat((semantic_output, labels), dim=1))\n",
    "\n",
    "\n",
    "        # print(f\"reconstructed_image_features.shape: {reconstructed_image_features.shape}\")\n",
    "\n",
    "    label_set = set()\n",
    "    for out in semantic_outputs:\n",
    "      label_set.add(out[1])\n",
    "    \n",
    "    print('n of unique labels: ', len(label_set))\n",
    "    print(label_set)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print('lossing')\n",
    "    # loss = total_loss(reconstructed_image_features, semantic_outputs, residual_output, labels) # + nn.NLLLoss()(semantic_output, labels) + nn.NLLLoss()(residual_output, labels)\n",
    "    loss = criterion(None, semantic_outputs, None, None)\n",
    "    print('done lossing')\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
